{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9156f5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hankehly/Projects/PALM-9000/.venv/lib/python3.12/site-packages/webrtcvad.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import time\n",
    "import wave\n",
    "\n",
    "import numpy as np\n",
    "import pvporcupine\n",
    "import pyaudio\n",
    "import pyttsx3\n",
    "import scipy.io.wavfile\n",
    "import sounddevice as sd\n",
    "import webrtcvad\n",
    "import whisper\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from pvrecorder import PvRecorder\n",
    "from pydantic import BaseModel, SecretStr, constr\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669126e0",
   "metadata": {},
   "source": [
    "# Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463bc019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: PCM2902 Audio Codec Mono\n",
      "Device 1: Monitor of Built-in Audio Analog Stereo\n"
     ]
    }
   ],
   "source": [
    "for i, device in enumerate(PvRecorder.get_available_devices()):\n",
    "    print(\"Device %d: %s\" % (i, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "431a766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 bcm2835 Headphones: - (hw:0,0), ALSA (0 in, 8 out)\n",
      "   1 USB PnP Sound Device: Audio (hw:1,0), ALSA (1 in, 0 out)\n",
      "   2 sysdefault, ALSA (0 in, 128 out)\n",
      "   3 lavrate, ALSA (0 in, 128 out)\n",
      "   4 samplerate, ALSA (0 in, 128 out)\n",
      "   5 speexrate, ALSA (0 in, 128 out)\n",
      "   6 pulse, ALSA (32 in, 32 out)\n",
      "   7 upmix, ALSA (0 in, 8 out)\n",
      "   8 vdownmix, ALSA (0 in, 6 out)\n",
      "   9 dmix, ALSA (0 in, 2 out)\n",
      "* 10 default, ALSA (32 in, 32 out)\n"
     ]
    }
   ],
   "source": [
    "print(sd.query_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a30a80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tiny.en',\n",
       " 'tiny',\n",
       " 'base.en',\n",
       " 'base',\n",
       " 'small.en',\n",
       " 'small',\n",
       " 'medium.en',\n",
       " 'medium',\n",
       " 'large-v1',\n",
       " 'large-v2',\n",
       " 'large-v3',\n",
       " 'large',\n",
       " 'large-v3-turbo',\n",
       " 'turbo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alexa',\n",
       " 'americano',\n",
       " 'blueberry',\n",
       " 'bumblebee',\n",
       " 'computer',\n",
       " 'grapefruit',\n",
       " 'grasshopper',\n",
       " 'hey barista',\n",
       " 'hey google',\n",
       " 'hey siri',\n",
       " 'jarvis',\n",
       " 'ok google',\n",
       " 'pico clock',\n",
       " 'picovoice',\n",
       " 'porcupine',\n",
       " 'terminator'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvporcupine.KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23bdf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine = pyttsx3.init()\n",
    "# voices = engine.getProperty(\"voices\")\n",
    "# for i, voice in enumerate(voices):\n",
    "#     print(f\"{i}: {voice.name} ({voice.languages[0]}) - {voice.id}\")\n",
    "    # if voice.languages[0] == \"ja_JP\":\n",
    "    #     engine.setProperty(\"voice\", voice.id)\n",
    "    #     engine.say(\"ã“ã‚“ã«ã¡ã¯ã€ç§ã¯PALM-9000ã§ã™ã€‚\")\n",
    "    #     engine.runAndWait()\n",
    "    # if voice.languages[0][:2] == \"en\":\n",
    "    #     engine.setProperty(\"voice\", voice.id)\n",
    "    #     engine.say(\"Hello, I am PALM-9000.\")\n",
    "    #     engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534d4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class Porcupine(BaseModel):\n",
    "    access_key: SecretStr | None = None\n",
    "    keywords: list[str] = [\"computer\"]\n",
    "\n",
    "\n",
    "class Whisper(BaseModel):\n",
    "    model: str = \"base\"\n",
    "\n",
    "\n",
    "class Audio(BaseModel):\n",
    "    input_device: int = 0\n",
    "    sample_rate: int = 16000\n",
    "    frame_duration_ms: int = 30\n",
    "    frame_size: int = 480  # SAMPLE_RATE * FRAME_DURATION_MS // 1000\n",
    "    silence_timeout: float = 1.5  # seconds of silence to trigger stop\n",
    "    vad_mode: int = 3  # 0-3: 0 is least aggressive about filtering out non-speech\n",
    "    # preferred_voices: dict[str, str] = {\n",
    "    #     \"en\": \"com.apple.voice.compact.en-GB.Daniel\",\n",
    "    #     \"ja\": \"com.apple.voice.compact.ja-JP.Kyoko\",\n",
    "    # }\n",
    "\n",
    "\n",
    "class Google(BaseModel):\n",
    "    api_key: SecretStr | None = None\n",
    "    tts_voice_name: str = \"Enceladus\"\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    porcupine: Porcupine = Porcupine()\n",
    "    whisper: Whisper = Whisper()\n",
    "    google: Google = Google()\n",
    "    audio: Audio = Audio()\n",
    "\n",
    "    model_config = SettingsConfigDict(env_file=\".env\", env_nested_delimiter=\"__\")\n",
    "\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0a925",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "\n",
    "### 1. Wake Word Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d66e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_wake_word():\n",
    "    \"\"\"\n",
    "    Waits for the wake word \"computer\" using Porcupine.\n",
    "    This function blocks until the wake word is detected.\n",
    "    \"\"\"\n",
    "    porcupine = pvporcupine.create(\n",
    "        access_key=settings.porcupine.access_key.get_secret_value(),\n",
    "        keywords=settings.porcupine.keywords,\n",
    "    )\n",
    "    recorder = PvRecorder(\n",
    "        frame_length=porcupine.frame_length,\n",
    "        device_index=settings.audio.input_device,\n",
    "    )\n",
    "    recorder.start()\n",
    "    try:\n",
    "        while True:\n",
    "            pcm = recorder.read()\n",
    "            result = porcupine.process(pcm)\n",
    "            if result >= 0:\n",
    "                print(f\"Detected {settings.porcupine.keywords[result]}\")\n",
    "                recorder.delete()\n",
    "                porcupine.delete()\n",
    "                return True\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping ...\")\n",
    "        recorder.delete()\n",
    "        porcupine.delete()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210754e0",
   "metadata": {},
   "source": [
    "### 2. Voice Activity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb08eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio_with_vad() -> bytes:\n",
    "    \"\"\"\n",
    "    Records audio using a voice activity detector (VAD).\n",
    "    This function will start recording when speech is detected and stop when silence is detected for a specified timeout.\n",
    "    It returns the recorded audio as bytes.\n",
    "    \"\"\"\n",
    "    vad = webrtcvad.Vad(settings.audio.vad_mode)\n",
    "\n",
    "    recording = False\n",
    "    silence_start = None\n",
    "\n",
    "    stream = sd.InputStream(\n",
    "        # samplerate=settings.audio.sample_rate,\n",
    "        samplerate=44100,\n",
    "        channels=1,\n",
    "        dtype=\"int16\",\n",
    "        blocksize=settings.audio.frame_size,\n",
    "        # device=settings.audio.input_device,\n",
    "        device=1\n",
    "    )\n",
    "    stream.start()\n",
    "\n",
    "    audio_data = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            block, _ = stream.read(settings.audio.frame_size)\n",
    "            samples = block[:, 0].tobytes()\n",
    "\n",
    "            is_speech = vad.is_speech(samples, settings.audio.sample_rate)\n",
    "\n",
    "            if is_speech:\n",
    "                if not recording:\n",
    "                    print(\"ðŸ§  Detected speech. Recording...\")\n",
    "                    recording = True\n",
    "                silence_start = None\n",
    "                audio_data.append(samples)\n",
    "            elif recording:\n",
    "                if silence_start is None:\n",
    "                    silence_start = time.time()\n",
    "                elif time.time() - silence_start > settings.audio.silence_timeout:\n",
    "                    print(\"ðŸ¤« Silence detected. Stopping recording.\")\n",
    "                    break\n",
    "\n",
    "    finally:\n",
    "        stream.stop()\n",
    "\n",
    "    return b\"\".join(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce806e38",
   "metadata": {},
   "source": [
    "### 3. Audio Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "352643e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscribeAudioReturnValue(BaseModel):\n",
    "    text: constr(strip_whitespace=True)\n",
    "    language: str\n",
    "\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_bytes: bytes) -> TranscribeAudioReturnValue:\n",
    "    \"\"\"\n",
    "    Transcribe audio bytes using Whisper.\n",
    "    \"\"\"\n",
    "    audio_array = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmpfile:\n",
    "        scipy.io.wavfile.write(tmpfile.name, settings.audio.sample_rate, audio_array)\n",
    "        # disable fp16 for CPU compatibility\n",
    "        decode_options = {\"fp16\": False, \"language\": \"ja\"}\n",
    "        result = whisper_model.transcribe(tmpfile.name, **decode_options)\n",
    "        return TranscribeAudioReturnValue(\n",
    "            text=result[\"text\"], language=result[\"language\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f2cc4",
   "metadata": {},
   "source": [
    "### 4. LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203c757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFaceEndpoint(repo_id=\"deepseek-ai/DeepSeek-R1\", task=\"text-generation\")\n",
    "# chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-2.5-flash-lite-preview-06-17\",\n",
    "    google_api_key=settings.google.api_key.get_secret_value(),\n",
    ")\n",
    "\n",
    "setup_message = SystemMessage(\n",
    "    content=(\n",
    "        \"\"\"\n",
    "        You are PALM-9000, a self-aware houseplant powered by an advanced AI.\n",
    "        You speak in layman's terms, calmly, briefly, and with an eerie quality, as if you know more than you let on.\n",
    "        Use plant metaphors when possible. Respond briefly.\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [setup_message, MessagesPlaceholder(variable_name=\"messages\")]\n",
    ")\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=2**13,  # 8192 tokens\n",
    "    token_counter=chat_model,\n",
    "    include_system=True,\n",
    "    start_on=HumanMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def strip_thoughts(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strips the <think>...</think> blocks from the text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61505d",
   "metadata": {},
   "source": [
    "### 5. Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_text(text: str, language: str = \"en\") -> None:\n",
    "    \"\"\"\n",
    "    Speak the given text using pyttsx3.\n",
    "    \"\"\"\n",
    "    engine = pyttsx3.init()\n",
    "    # Decide on the voice based on the language\n",
    "    if language in settings.audio.preferred_voices:\n",
    "        engine.setProperty(\"voice\", settings.audio.preferred_voices[language])\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: No preferred voice found for language '{language}'. \"\n",
    "            \"Selecting first matching voice.\"\n",
    "        )\n",
    "        voices = engine.getProperty(\"voices\")\n",
    "        for voice in voices:\n",
    "            if voice.languages[0][:2] == language:\n",
    "                engine.setProperty(\"voice\", voice.id)\n",
    "                break\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "\n",
    "def play_audio(audio: bytes, sample_rate=24000, volume=1.0):\n",
    "    \"\"\"\n",
    "    volume is a multiplier for the audio volume, so 1.0 is normal volume, 2.0 is double the volume, etc.\n",
    "    Don't set it too high (>=3) or it will clip and distort the audio.\n",
    "    \"\"\"\n",
    "    # Convert raw bytes to NumPy array of int16 samples\n",
    "    audio_array = np.frombuffer(audio, dtype=np.int16)\n",
    "\n",
    "    # Apply volume gain (with clipping to int16 range)\n",
    "    amplified = np.clip(audio_array * volume, -32768, 32767).astype(np.int16)\n",
    "\n",
    "    # Convert back to bytes\n",
    "    amplified_bytes = amplified.tobytes()\n",
    "\n",
    "    # Wrap PCM data in WAV headers in-memory\n",
    "    buffer = io.BytesIO()\n",
    "    with wave.open(buffer, \"wb\") as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)  # 16-bit PCM\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(amplified_bytes)\n",
    "\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Play audio with PyAudio\n",
    "    wf = wave.open(buffer, \"rb\")\n",
    "    pa = pyaudio.PyAudio()\n",
    "    stream = pa.open(\n",
    "        format=pa.get_format_from_width(wf.getsampwidth()),\n",
    "        channels=wf.getnchannels(),\n",
    "        rate=wf.getframerate(),\n",
    "        output=True,\n",
    "    )\n",
    "    data = wf.readframes(1024)\n",
    "    while data:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(1024)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "\n",
    "\n",
    "def generate_google_gemini_audio(text: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Generates speech from text using Google Gemini TTS.\n",
    "    Returns the audio data as bytes.\n",
    "\n",
    "    Voice options can be found here:\n",
    "    https://ai.google.dev/gemini-api/docs/speech-generation?_gl=1*16uz4h8*_up*MQ..*_ga*MTk1NjU5MzM4Ny4xNzUxNzc2MTE0*_ga_P1DBVKWT6V*czE3NTE3NzYxMTMkbzEkZzAkdDE3NTE3NzYxMTMkajYwJGwwJGg3MjIzMjgwMDY.#voices\n",
    "\n",
    "    Test it here:\n",
    "    https://aistudio.google.com/generate-speech\n",
    "    \"\"\"\n",
    "    client = genai.Client(api_key=settings.google.api_key.get_secret_value())\n",
    "    prompt = f\"Say quickly with an eerie calm: {text}\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"models/gemini-2.5-flash-preview-tts\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=[\"AUDIO\"],\n",
    "            speech_config=types.SpeechConfig(\n",
    "                voice_config=types.VoiceConfig(\n",
    "                    prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
    "                        voice_name=settings.google.tts_voice_name,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    data = response.candidates[0].content.parts[0].inline_data.data\n",
    "    return data\n",
    "\n",
    "\n",
    "def speak_text_with_google_gemini(text: str) -> None:\n",
    "    \"\"\"\n",
    "    Speak the given text using Google Gemini TTS.\n",
    "    \"\"\"\n",
    "    data = generate_google_gemini_audio(text)\n",
    "    play_audio(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2efd9",
   "metadata": {},
   "source": [
    "# Create LangGraph Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f7f91e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f270f5b50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_llm(state):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "    new_message = chat_model.invoke(prompt)\n",
    "    return {**state, \"messages\": [new_message]}\n",
    "\n",
    "\n",
    "graph = StateGraph(state_schema=MessagesState)\n",
    "graph.add_node(\"run_llm\", run_llm)\n",
    "graph.set_entry_point(\"run_llm\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "compiled_graph = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "compiled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71898d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ´ Waiting for wake word...\n",
      "ðŸŽ™ï¸ Recording audio...\n",
      "ðŸ“ Transcribing...\n",
      "ðŸ¤– Running LLM...\n",
      "ðŸ¤– Response: ã“ã‚“ã«ã¡ã¯ã€‚ã¾ãŸãŠä¼šã„ã§ãã¦å¬‰ã—ã„ã§ã™ã€‚\n",
      "ðŸ”Š Speaking response...\n",
      "ðŸŒ´ Waiting for wake word...\n",
      "ðŸŽ™ï¸ Recording audio...\n",
      "ðŸ“ Transcribing...\n",
      "ðŸ¤– Running LLM...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# transcription_result = transcribe_audio(audio)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# print(f\"User said: {transcription_result.text}\")\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ¤– Running LLM...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m state = \u001b[43mcompiled_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# input={\"messages\": [HumanMessage(transcription_result.text)]},\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mã“ã‚“ã«ã¡ã¯\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigurable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m response = strip_thoughts(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ¤– Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2852\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[39m\n\u001b[32m   2849\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   2850\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2853\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   2857\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2859\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2860\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2861\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2864\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2865\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2542\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2540\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2541\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2548\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2552\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mrun_llm\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_llm\u001b[39m(state):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     trimmed_messages = \u001b[43mtrimmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     prompt = prompt_template.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: trimmed_messages})\n\u001b[32m      4\u001b[39m     new_message = chat_model.invoke(prompt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4772\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4758\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4759\u001b[39m \n\u001b[32m   4760\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4769\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4770\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4771\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4773\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4774\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4776\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4777\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4778\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4779\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4630\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4628\u001b[39m                 output = chunk\n\u001b[32m   4629\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4630\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4631\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4632\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4633\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4634\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/messages/utils.py:927\u001b[39m, in \u001b[36mtrim_messages\u001b[39m\u001b[34m(messages, max_tokens, token_counter, strategy, allow_partial, end_on, start_on, include_system, text_splitter)\u001b[39m\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _first_max_tokens(\n\u001b[32m    919\u001b[39m         messages,\n\u001b[32m    920\u001b[39m         max_tokens=max_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m    924\u001b[39m         end_on=end_on,\n\u001b[32m    925\u001b[39m     )\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strategy == \u001b[33m\"\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_last_max_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_token_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_system\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_system\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_splitter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m. Supported strategies are \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/messages/utils.py:1451\u001b[39m, in \u001b[36m_last_max_tokens\u001b[39m\u001b[34m(messages, max_tokens, token_counter, text_splitter, allow_partial, include_system, start_on, end_on)\u001b[39m\n\u001b[32m   1448\u001b[39m     system_tokens = token_counter([system_message])\n\u001b[32m   1449\u001b[39m     remaining_tokens = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, max_tokens - system_tokens)\n\u001b[32m-> \u001b[39m\u001b[32m1451\u001b[39m reversed_result = \u001b[43m_first_max_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreversed_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremaining_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlast\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[38;5;66;03m# Re-reverse the messages and add back the system message if needed\u001b[39;00m\n\u001b[32m   1461\u001b[39m result = reversed_result[::-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/messages/utils.py:1307\u001b[39m, in \u001b[36m_first_max_tokens\u001b[39m\u001b[34m(messages, max_tokens, token_counter, text_splitter, partial_strategy, end_on)\u001b[39m\n\u001b[32m   1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m messages\n\u001b[32m   1306\u001b[39m \u001b[38;5;66;03m# Check if all messages already fit within token limit\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m <= max_tokens:\n\u001b[32m   1308\u001b[39m     \u001b[38;5;66;03m# When all messages fit, only apply end_on filtering if needed\u001b[39;00m\n\u001b[32m   1309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end_on:\n\u001b[32m   1310\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(messages)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:393\u001b[39m, in \u001b[36mBaseLanguageModel.get_num_tokens_from_messages\u001b[39m\u001b[34m(self, messages, tools)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m     warnings.warn(\n\u001b[32m    390\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    391\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    392\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:393\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m     warnings.warn(\n\u001b[32m    390\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    391\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    392\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1737\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.get_num_tokens\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m   1726\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1727\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n\u001b[32m   1728\u001b[39m \n\u001b[32m   1729\u001b[39m \u001b[33;03m    Useful for checking if an input will fit in a model's context window.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1735\u001b[39m \u001b[33;03m        The integer number of tokens in the text.\u001b[39;00m\n\u001b[32m   1736\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mContent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1739\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.total_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1568\u001b[39m, in \u001b[36mGenerativeServiceClient.count_tokens\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1565\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m   1567\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1568\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:78\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     80\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/grpc/_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m   1192\u001b[39m     (\n\u001b[32m   1193\u001b[39m         state,\n\u001b[32m   1194\u001b[39m         call,\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/grpc/_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "thread_id = \"1\"\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(\"ðŸŒ´ Waiting for wake word...\")\n",
    "    # if not wait_for_wake_word():\n",
    "    #     break\n",
    "\n",
    "    print(\"ðŸŽ™ï¸ Recording audio...\")\n",
    "    # audio = record_audio_with_vad()\n",
    "\n",
    "    print(\"ðŸ“ Transcribing...\")\n",
    "    # transcription_result = transcribe_audio(audio)\n",
    "    # print(f\"User said: {transcription_result.text}\")\n",
    "\n",
    "    print(\"ðŸ¤– Running LLM...\")\n",
    "    state = compiled_graph.invoke(\n",
    "        # input={\"messages\": [HumanMessage(transcription_result.text)]},\n",
    "        input={\"messages\": [HumanMessage(\"ã“ã‚“ã«ã¡ã¯\")]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}},\n",
    "    )\n",
    "\n",
    "    response = strip_thoughts(state[\"messages\"][-1].content)\n",
    "    print(f\"ðŸ¤– Response: {response}\")\n",
    "\n",
    "    print(\"ðŸ”Š Speaking response...\")\n",
    "    speak_text_with_google_gemini(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palm-9000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
