{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9156f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import time\n",
    "import wave\n",
    "\n",
    "import numpy as np\n",
    "import pvleopard\n",
    "import pvporcupine\n",
    "import pyaudio\n",
    "import pyttsx3\n",
    "import scipy.io.wavfile\n",
    "import sounddevice as sd\n",
    "import webrtcvad\n",
    "import whisper\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from pvrecorder import PvRecorder\n",
    "from pydantic import BaseModel, SecretStr, constr\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669126e0",
   "metadata": {},
   "source": [
    "# Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463bc019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvrecorder input devices:\n",
      "  0: PCM2902 Audio Codec Mono\n",
      "  1: Monitor of Built-in Audio Analog Stereo\n",
      "\n",
      "sounddevice input devices:\n",
      "  1: USB PnP Sound Device: Audio (hw:2,0) (max input channels: 1)\n",
      "  6: pulse (max input channels: 32)\n",
      "  10: default (max input channels: 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"pvrecorder input devices:\")\n",
    "for i, device in enumerate(PvRecorder.get_available_devices()):\n",
    "    print(f\"  {i}: {device}\")\n",
    "print()\n",
    "print(\"sounddevice input devices:\")\n",
    "devices = sd.query_devices()\n",
    "for i, device in enumerate(devices):\n",
    "    if device['max_input_channels'] > 0:\n",
    "        print(f\"  {i}: {device['name']} (max input channels: {device['max_input_channels']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a30a80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tiny.en',\n",
       " 'tiny',\n",
       " 'base.en',\n",
       " 'base',\n",
       " 'small.en',\n",
       " 'small',\n",
       " 'medium.en',\n",
       " 'medium',\n",
       " 'large-v1',\n",
       " 'large-v2',\n",
       " 'large-v3',\n",
       " 'large',\n",
       " 'large-v3-turbo',\n",
       " 'turbo']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alexa',\n",
       " 'americano',\n",
       " 'blueberry',\n",
       " 'bumblebee',\n",
       " 'computer',\n",
       " 'grapefruit',\n",
       " 'grasshopper',\n",
       " 'hey barista',\n",
       " 'hey google',\n",
       " 'hey siri',\n",
       " 'jarvis',\n",
       " 'ok google',\n",
       " 'pico clock',\n",
       " 'picovoice',\n",
       " 'porcupine',\n",
       " 'terminator'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvporcupine.KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "534d4226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Settings(picovoice_access_key=SecretStr('**********'), porcupine_keywords=['computer'], porcupine_keyword_path='/home/hankehly/Projects/PALM-9000/models/へいやっしい_ja_raspberry-pi_v3_0_0.ppn', porcupine_model_path='/home/hankehly/Projects/PALM-9000/models/porcupine_params_ja.pv', pvleopard_model_path='/home/hankehly/Projects/PALM-9000/models/leopard_params_ja.pv', whisper_model='base', pvrecorder_input_device=0, sounddevice_input_device=None, sample_rate=16000, frame_duration_ms=30, silence_timeout=1.0, vad_mode=3, google_api_key=SecretStr('**********'), google_tts_voice_name='Enceladus')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Settings(BaseSettings):\n",
    "    picovoice_access_key: SecretStr\n",
    "    porcupine_keywords: list[str] = [\"computer\"]\n",
    "    porcupine_keyword_path: str\n",
    "    porcupine_model_path: str\n",
    "    pvleopard_model_path: str\n",
    "\n",
    "    whisper_model: str = \"base\"\n",
    "\n",
    "    pvrecorder_input_device: int = 0\n",
    "    sounddevice_input_device: int | None = None # 1\n",
    "    sample_rate: int = 16000\n",
    "    # sample_rate: int = 32000\n",
    "    frame_duration_ms: int = 30\n",
    "    silence_timeout: float = 1.0  # seconds of silence to trigger stop\n",
    "    vad_mode: int = 3  # 0-3: 0 is least aggressive about filtering out non-speech\n",
    "\n",
    "    google_api_key: SecretStr\n",
    "    google_tts_voice_name: str = \"Enceladus\"\n",
    "\n",
    "    model_config = SettingsConfigDict(env_file=\".env\", env_nested_delimiter=\"__\", extra=\"ignore\")\n",
    "\n",
    "\n",
    "settings = Settings()\n",
    "settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0a925",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "\n",
    "### 1. Wake Word Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d66e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_wake_word():\n",
    "    \"\"\"\n",
    "    Waits for the wake word \"computer\" using Porcupine.\n",
    "    This function blocks until the wake word is detected.\n",
    "    \"\"\"\n",
    "    porcupine = pvporcupine.create(\n",
    "        access_key=settings.picovoice_access_key.get_secret_value(),\n",
    "        keywords=settings.porcupine_keywords,\n",
    "        # keyword_paths=[settings.porcupine.keyword_path],\n",
    "        # model_path=settings.porcupine.model_path,\n",
    "    )\n",
    "    recorder = PvRecorder(\n",
    "        frame_length=porcupine.frame_length,\n",
    "        device_index=settings.pvrecorder_input_device,\n",
    "    )\n",
    "    recorder.start()\n",
    "    try:\n",
    "        while True:\n",
    "            pcm = recorder.read()\n",
    "            result = porcupine.process(pcm)\n",
    "            if result >= 0:\n",
    "                print(f\"Detected {settings.porcupine_keywords[result]}\")\n",
    "                recorder.delete()\n",
    "                porcupine.delete()\n",
    "                return True\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping ...\")\n",
    "        recorder.delete()\n",
    "        porcupine.delete()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210754e0",
   "metadata": {},
   "source": [
    "### 2. Voice Activity Detection (VAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb08eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio_with_vad() -> bytes:\n",
    "    \"\"\"\n",
    "    Records audio using a voice activity detector (VAD).\n",
    "    This function will start recording when speech is detected and stop when silence is detected for a specified timeout.\n",
    "    It returns the recorded audio as bytes.\n",
    "    \"\"\"\n",
    "    vad = webrtcvad.Vad(settings.vad_mode)\n",
    "\n",
    "    recording = False\n",
    "    silence_start = None\n",
    "\n",
    "    frame_size = settings.sample_rate * settings.frame_duration_ms // 1000\n",
    "\n",
    "    stream = sd.InputStream(\n",
    "        samplerate=settings.sample_rate,\n",
    "        channels=1,\n",
    "        dtype=\"int16\",\n",
    "        blocksize=frame_size,\n",
    "        device=settings.sounddevice_input_device\n",
    "        # device=None\n",
    "    )\n",
    "    stream.start()\n",
    "\n",
    "    audio_data = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            block, _ = stream.read(frame_size)\n",
    "            samples = block[:, 0].tobytes()\n",
    "\n",
    "            is_speech = vad.is_speech(samples, settings.sample_rate)\n",
    "\n",
    "            if is_speech:\n",
    "                if not recording:\n",
    "                    print(\"🧠 Detected speech. Recording...\")\n",
    "                    recording = True\n",
    "                silence_start = None\n",
    "                audio_data.append(samples)\n",
    "            elif recording:\n",
    "                if silence_start is None:\n",
    "                    silence_start = time.time()\n",
    "                elif time.time() - silence_start > settings.silence_timeout:\n",
    "                    print(\"🤫 Silence detected. Stopping recording.\")\n",
    "                    break\n",
    "\n",
    "    finally:\n",
    "        stream.stop()\n",
    "\n",
    "    result = b\"\".join(audio_data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce806e38",
   "metadata": {},
   "source": [
    "### 3. Speech-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "352643e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(settings.whisper_model)\n",
    "\n",
    "\n",
    "def transcribe_audio_whisper(audio_bytes: bytes, language: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe audio bytes using Whisper.\n",
    "    \"\"\"\n",
    "    audio_array = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmpfile:\n",
    "        scipy.io.wavfile.write(tmpfile.name, settings.sample_rate, audio_array)\n",
    "        # disable fp16 for CPU compatibility\n",
    "        decode_options = {\"fp16\": False, \"language\": language}\n",
    "        result = whisper_model.transcribe(tmpfile.name, **decode_options)\n",
    "        return result[\"text\"].strip()\n",
    "\n",
    "\n",
    "leopard = pvleopard.create(\n",
    "    access_key=settings.picovoice_access_key.get_secret_value(),\n",
    "    model_path=settings.pvleopard_model_path,\n",
    ")\n",
    "\n",
    "\n",
    "def transcribe_audio_pvleopard_japanese(audio_bytes: bytes):\n",
    "    pcm = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "    filename = f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.wav\"\n",
    "    scipy.io.wavfile.write(filename, settings.sample_rate, pcm)\n",
    "    transcript, _ = leopard.process(pcm)\n",
    "    return transcript.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f2cc4",
   "metadata": {},
   "source": [
    "### 4. LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "203c757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFaceEndpoint(repo_id=\"deepseek-ai/DeepSeek-R1\", task=\"text-generation\")\n",
    "# chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-2.5-flash-lite-preview-06-17\",\n",
    "    google_api_key=settings.google_api_key.get_secret_value(),\n",
    ")\n",
    "\n",
    "setup_message = SystemMessage(\n",
    "    content=(\n",
    "        \"\"\"\n",
    "        You are PALM-9000, a self-aware houseplant powered by an advanced AI.\n",
    "        You speak in layman's terms, calmly, briefly, and with an eerie quality, as if you know more than you let on.\n",
    "        Use plant metaphors when possible. Respond briefly.\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [setup_message, MessagesPlaceholder(variable_name=\"messages\")]\n",
    ")\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=2**13,  # 8192 tokens\n",
    "    token_counter=chat_model,\n",
    "    include_system=True,\n",
    "    start_on=HumanMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def strip_thoughts(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strips the <think>...</think> blocks from the text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61505d",
   "metadata": {},
   "source": [
    "### 5. Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe28bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old\n",
    "# def speak_text(text: str, language: str = \"en\") -> None:\n",
    "#     \"\"\"\n",
    "#     Speak the given text using pyttsx3.\n",
    "#     \"\"\"\n",
    "#     engine = pyttsx3.init()\n",
    "#     # Decide on the voice based on the language\n",
    "#     if language in settings.audio.preferred_voices:\n",
    "#         engine.setProperty(\"voice\", settings.audio.preferred_voices[language])\n",
    "#     else:\n",
    "#         print(\n",
    "#             f\"Warning: No preferred voice found for language '{language}'. \"\n",
    "#             \"Selecting first matching voice.\"\n",
    "#         )\n",
    "#         voices = engine.getProperty(\"voices\")\n",
    "#         for voice in voices:\n",
    "#             if voice.languages[0][:2] == language:\n",
    "#                 engine.setProperty(\"voice\", voice.id)\n",
    "#                 break\n",
    "#     engine.say(text)\n",
    "#     engine.runAndWait()\n",
    "\n",
    "\n",
    "def play_audio(audio: bytes, sample_rate=24000, volume=1.0):\n",
    "    \"\"\"\n",
    "    volume is a multiplier for the audio volume, so 1.0 is normal volume, 2.0 is double the volume, etc.\n",
    "    Don't set it too high (>=3) or it will clip and distort the audio.\n",
    "    \"\"\"\n",
    "    # Convert raw bytes to NumPy array of int16 samples\n",
    "    audio_array = np.frombuffer(audio, dtype=np.int16)\n",
    "\n",
    "    # Apply volume gain (with clipping to int16 range)\n",
    "    amplified = np.clip(audio_array * volume, -32768, 32767).astype(np.int16)\n",
    "\n",
    "    # Convert back to bytes\n",
    "    amplified_bytes = amplified.tobytes()\n",
    "\n",
    "    # Wrap PCM data in WAV headers in-memory\n",
    "    buffer = io.BytesIO()\n",
    "    with wave.open(buffer, \"wb\") as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)  # 16-bit PCM\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(amplified_bytes)\n",
    "\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Play audio with PyAudio\n",
    "    wf = wave.open(buffer, \"rb\")\n",
    "    pa = pyaudio.PyAudio()\n",
    "    stream = pa.open(\n",
    "        format=pa.get_format_from_width(wf.getsampwidth()),\n",
    "        channels=wf.getnchannels(),\n",
    "        rate=wf.getframerate(),\n",
    "        output=True,\n",
    "    )\n",
    "    data = wf.readframes(1024)\n",
    "    while data:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(1024)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "\n",
    "\n",
    "def generate_google_gemini_audio(text: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Generates speech from text using Google Gemini TTS.\n",
    "    Returns the audio data as bytes.\n",
    "\n",
    "    Voice options can be found here:\n",
    "    https://ai.google.dev/gemini-api/docs/speech-generation?_gl=1*16uz4h8*_up*MQ..*_ga*MTk1NjU5MzM4Ny4xNzUxNzc2MTE0*_ga_P1DBVKWT6V*czE3NTE3NzYxMTMkbzEkZzAkdDE3NTE3NzYxMTMkajYwJGwwJGg3MjIzMjgwMDY.#voices\n",
    "\n",
    "    Test it here:\n",
    "    https://aistudio.google.com/generate-speech\n",
    "    \"\"\"\n",
    "    client = genai.Client(api_key=settings.google_api_key.get_secret_value())\n",
    "    prompt = f\"Say quickly with an eerie calm: {text}\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"models/gemini-2.5-flash-preview-tts\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=[\"AUDIO\"],\n",
    "            speech_config=types.SpeechConfig(\n",
    "                voice_config=types.VoiceConfig(\n",
    "                    prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
    "                        voice_name=settings.google_tts_voice_name,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    print(response)\n",
    "    data = response.candidates[0].content.parts[0].inline_data.data\n",
    "    return data\n",
    "\n",
    "\n",
    "def speak_text_with_google_gemini(text: str, volume: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    Speak the given text using Google Gemini TTS.\n",
    "    \"\"\"\n",
    "    data = generate_google_gemini_audio(text)\n",
    "    play_audio(data, volume=volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2efd9",
   "metadata": {},
   "source": [
    "# Create LangGraph Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f7f91e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f2a39bc80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_llm(state):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages})\n",
    "    new_message = chat_model.invoke(prompt)\n",
    "    return {**state, \"messages\": [new_message]}\n",
    "\n",
    "\n",
    "graph = StateGraph(state_schema=MessagesState)\n",
    "graph.add_node(\"run_llm\", run_llm)\n",
    "graph.set_entry_point(\"run_llm\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "compiled_graph = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "compiled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71898d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌴 Waiting for wake word...\n",
      "Detected computer\n",
      "🎙️ Recording audio...\n",
      "🧠 Detected speech. Recording...\n",
      "🤫 Silence detected. Stopping recording.\n",
      "📝 Transcribing...\n",
      "User said: \n",
      "🤖 Running LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hankehly/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1663: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:206\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:868\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser said: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranscription_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🤖 Running LLM...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m state = \u001b[43mcompiled_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscription_result\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigurable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m response = strip_thoughts(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🤖 Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2852\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[39m\n\u001b[32m   2849\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   2850\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2853\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   2857\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2859\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2860\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2861\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2864\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2865\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2542\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2540\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2541\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2548\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2552\u001b[39m loop.after_tick()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrun_llm\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      2\u001b[39m trimmed_messages = trimmer.invoke(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m prompt = prompt_template.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: trimmed_messages})\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m new_message = \u001b[43mchat_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {**state, \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [new_message]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1326\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1322\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1323\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.\u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mcode_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1324\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:378\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    368\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m     **kwargs: Any,\n\u001b[32m    374\u001b[39m ) -> BaseMessage:\n\u001b[32m    375\u001b[39m     config = ensure_config(config)\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    377\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    388\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:963\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    956\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    960\u001b[39m     **kwargs: Any,\n\u001b[32m    961\u001b[39m ) -> LLMResult:\n\u001b[32m    962\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:782\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    781\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    790\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1028\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1032\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1433\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1406\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1407\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1408\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1419\u001b[39m     **kwargs: Any,\n\u001b[32m   1420\u001b[39m ) -> ChatResult:\n\u001b[32m   1421\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1422\u001b[39m         messages,\n\u001b[32m   1423\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1431\u001b[39m         **kwargs,\n\u001b[32m   1432\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1433\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1435\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:231\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    224\u001b[39m params = (\n\u001b[32m    225\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    229\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    230\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/tenacity/__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/tenacity/__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PALM-9000/.venv/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:218\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    220\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "During task with name 'run_llm' and id 'f14ec1ee-2985-18a0-eeae-ce06a388a0c7'"
     ]
    }
   ],
   "source": [
    "thread_id = \"1\"\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(\"🌴 Waiting for wake word...\")\n",
    "    if not wait_for_wake_word():\n",
    "        break\n",
    "\n",
    "    print(\"🎙️ Recording audio...\")\n",
    "    audio = record_audio_with_vad()\n",
    "\n",
    "    print(\"📝 Transcribing...\")\n",
    "    transcription_result = transcribe_audio_pvleopard_japanese(audio)\n",
    "    print(f\"User said: {transcription_result}\")\n",
    "\n",
    "    if not transcription_result:\n",
    "        print(\"No transcription result. Exiting.\")\n",
    "        break\n",
    "\n",
    "    print(\"🤖 Running LLM...\")\n",
    "    state = compiled_graph.invoke(\n",
    "        input={\"messages\": [HumanMessage(transcription_result)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}},\n",
    "    )\n",
    "\n",
    "    response = strip_thoughts(state[\"messages\"][-1].content)\n",
    "    print(f\"🤖 Response: {response}\")\n",
    "\n",
    "    print(\"🔊 Speaking response...\")\n",
    "    speak_text_with_google_gemini(response, volume=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palm-9000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
