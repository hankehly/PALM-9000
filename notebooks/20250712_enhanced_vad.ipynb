{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd39aca3",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook explores an enhanced implementation of the VAD processing. The problem was that we were triggering audio recording too early due to false positives. This new implementation uses a sliding window approach (taken from the py-webrtcvad examples file) to reduce false positives thereby improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import webrtcvad\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "\n",
    "class Frame:\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Frame(timestamp={self.timestamp}, duration={self.duration}, bytes_length={len(self.bytes)})\"\n",
    "\n",
    "\n",
    "def audio_frame_generator(\n",
    "    *,\n",
    "    frame_duration_ms: int,\n",
    "    sample_rate: int,\n",
    "    device: int,\n",
    ") -> Iterable[Frame]:\n",
    "    \"\"\"\n",
    "    Generate audio frames from the microphone.\n",
    "    \"\"\"\n",
    "    duration = frame_duration_ms / 1000.0  # E.g., 0.03 seconds\n",
    "    frame_size = int(sample_rate * duration)  # E.g., 44100 * 0.03 = 1323 samples\n",
    "    timestamp = 0.0\n",
    "    with sd.InputStream(\n",
    "        samplerate=sample_rate,\n",
    "        channels=1,\n",
    "        dtype=\"int16\",\n",
    "        blocksize=frame_size,\n",
    "        device=device,\n",
    "    ) as stream:\n",
    "        while True:\n",
    "            audio = stream.read(frame_size)[0]\n",
    "            yield Frame(audio.tobytes(), timestamp, duration)\n",
    "            timestamp += duration\n",
    "\n",
    "\n",
    "def resample(\n",
    "    audio: np.ndarray, original_sample_rate: int, target_sample_rate: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This is the equivalent of calling:\n",
    "    resample_poly(audio, target_sample_rate, original_sample_rate)\n",
    "\n",
    "    But the program will use less compute resources if we reduce the\n",
    "    ratio 44100:16000 to 441:160 with np.gcd (Greatest Common Divisor)\n",
    "    which finds the largest integer that evenly divides two numbers.\n",
    "    \"\"\"\n",
    "    gcd = np.gcd(original_sample_rate, target_sample_rate)\n",
    "    return resample_poly(audio, target_sample_rate // gcd, original_sample_rate // gcd)\n",
    "\n",
    "\n",
    "def resample_frames(\n",
    "    frames: Iterable[Frame],\n",
    "    original_sample_rate: int,\n",
    "    target_sample_rate: int,\n",
    ") -> Iterable[Frame]:\n",
    "    \"\"\"\n",
    "    A generator that wraps an audio frame generator and resamples\n",
    "    the audio frames.\n",
    "    \"\"\"\n",
    "    for frame in frames:\n",
    "        audio = np.frombuffer(frame.bytes, dtype=np.int16)\n",
    "        resampled_audio = resample(audio, original_sample_rate, target_sample_rate)\n",
    "        # We are changing the sample rate, not the bit-depth.\n",
    "        # Therefore we can keep the same dtype.\n",
    "        resampled_audio = np.clip(resampled_audio, -32768, 32767).astype(np.int16)\n",
    "        yield Frame(resampled_audio.tobytes(), frame.timestamp, frame.duration)\n",
    "\n",
    "\n",
    "def vad_collector(\n",
    "    *,\n",
    "    sample_rate: int,\n",
    "    frame_duration_ms: int,\n",
    "    padding_duration_ms: int,\n",
    "    vad: webrtcvad.Vad,\n",
    "    frames: Iterable[Frame],\n",
    ") -> Iterable[bytes]:\n",
    "    \"\"\"\n",
    "    Implementation taken from the example in the py-webrtcvad\n",
    "    https://github.com/wiseman/py-webrtcvad/blob/master/example.py\n",
    "    \"\"\"\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "    # We use a deque for our sliding window/ring buffer.\n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
    "    # NOTTRIGGERED state.\n",
    "    triggered = False\n",
    "\n",
    "    voiced_frames = []\n",
    "    for frame in frames:\n",
    "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
    "\n",
    "        print(\"1\" if is_speech else \"0\")\n",
    "        if not triggered:\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
    "            # the ring buffer are voiced frames, then enter the\n",
    "            # TRIGGERED state.\n",
    "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = True\n",
    "                print(\"+(%s)\" % (ring_buffer[0][0].timestamp,))\n",
    "                # We want to yield all the audio we see from now until\n",
    "                # we are NOTTRIGGERED, but we have to start with the\n",
    "                # audio that's already in the ring buffer.\n",
    "                for f, s in ring_buffer:\n",
    "                    voiced_frames.append(f)\n",
    "                ring_buffer.clear()\n",
    "        else:\n",
    "            # We're in the TRIGGERED state, so collect the audio data\n",
    "            # and add it to the ring buffer.\n",
    "            voiced_frames.append(frame)\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "            # If more than 90% of the frames in the ring buffer are\n",
    "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
    "            # audio we've collected.\n",
    "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
    "                print(\"-(%s)\" % frame)\n",
    "                triggered = False\n",
    "                yield b\"\".join([f.bytes for f in voiced_frames])\n",
    "                ring_buffer.clear()\n",
    "                voiced_frames = []\n",
    "    if triggered:\n",
    "        print(\"-(%s)\" % frame)\n",
    "    print(\"\\n\")\n",
    "    # If we have any leftover voiced audio when we run out of input,\n",
    "    # yield it.\n",
    "    if voiced_frames:\n",
    "        yield b\"\".join([f.bytes for f in voiced_frames])\n",
    "\n",
    "\n",
    "frame_duration_ms = 30  # Duration of each frame in milliseconds\n",
    "mic_sample_rate = 44100  # Original sample rate\n",
    "vad_sample_rate = 32000  # Sample rate for VAD processing\n",
    "device = 1  # Device index for the microphone (change as needed)\n",
    "\n",
    "raw_frame_generator = audio_frame_generator(\n",
    "    frame_duration_ms=frame_duration_ms,\n",
    "    sample_rate=mic_sample_rate,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "resampled_frames = resample_frames(\n",
    "    raw_frame_generator,\n",
    "    original_sample_rate=mic_sample_rate,\n",
    "    target_sample_rate=vad_sample_rate,\n",
    ")\n",
    "\n",
    "vad = webrtcvad.Vad(3)\n",
    "voiced_audio_generator = vad_collector(\n",
    "    sample_rate=vad_sample_rate,\n",
    "    frame_duration_ms=frame_duration_ms,\n",
    "    padding_duration_ms=300,\n",
    "    vad=vad,\n",
    "    frames=resampled_frames,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palm-9000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
