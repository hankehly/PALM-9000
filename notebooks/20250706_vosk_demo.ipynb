{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7092955b",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Testing out vosk for speech recognition on a Raspberry Pi 4B. The code records audio and transcribes it using the Vosk model for Japanese language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fc1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://alphacephei.com/vosk/models/vosk-model-ja-0.22.zip\n",
    "! unzip vosk-model-ja-0.22.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ec66f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import sounddevice as sd\n",
    "import json\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "588099ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available input devices:\n",
      "{'name': 'USB PnP Sound Device: Audio (hw:2,0)', 'index': 1, 'hostapi': 0, 'max_input_channels': 1, 'max_output_channels': 0, 'default_low_input_latency': 0.008684807256235827, 'default_low_output_latency': -1.0, 'default_high_input_latency': 0.034829931972789115, 'default_high_output_latency': -1.0, 'default_samplerate': 44100.0}\n",
      "{'name': 'pulse', 'index': 6, 'hostapi': 0, 'max_input_channels': 32, 'max_output_channels': 32, 'default_low_input_latency': 0.008684807256235827, 'default_low_output_latency': 0.008684807256235827, 'default_high_input_latency': 0.034807256235827665, 'default_high_output_latency': 0.034807256235827665, 'default_samplerate': 44100.0}\n",
      "{'name': 'default', 'index': 10, 'hostapi': 0, 'max_input_channels': 32, 'max_output_channels': 32, 'default_low_input_latency': 0.008684807256235827, 'default_low_output_latency': 0.008684807256235827, 'default_high_input_latency': 0.034807256235827665, 'default_high_output_latency': 0.034807256235827665, 'default_samplerate': 44100.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Available input devices:\")\n",
    "devices = sd.query_devices()\n",
    "for i, device in enumerate(devices):\n",
    "    if device['max_input_channels'] > 0:\n",
    "        print(device)\n",
    "        # print(f\"  {i}: {device['name']} (max input channels: {device['max_input_channels']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e294e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/hankehly/Projects/PALM-9000/models/vosk-model-ja-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /home/hankehly/Projects/PALM-9000/models/vosk-model-ja-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:297) Loading words from /home/hankehly/Projects/PALM-9000/models/vosk-model-ja-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/hankehly/Projects/PALM-9000/models/vosk-model-ja-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:315) Loading subtract G.fst model from /home/hankehly/Projects/PALM-9000/models/vosk-model-ja-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:317) Loading CARPA model from /home/hankehly/Projects/PALM-9000/models/vosk-model-ja-0.22/rescore/G.carpa\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 44100  # Adjusted to match the model's expected sample rate\n",
    "frame_duration_ms = 30\n",
    "frame_size = sample_rate * frame_duration_ms // 1000\n",
    "device = 1\n",
    "\n",
    "model = Model(\"/home/hankehly/Projects/PALM-9000/models/vosk-model-ja-0.22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662fc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎙️ Recording for 4.0 seconds...\n",
      "✅ Done recording.\n",
      "FinalResult: {'text': 'こんにちは 今日 は 日曜 日 です'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'こんにちは 今日 は 日曜 日 です'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def record_audio(\n",
    "    duration_sec: float = 4.0, sample_rate: int = 44100, device: int | None = None\n",
    ") -> bytes:\n",
    "    \"\"\"\n",
    "    Record audio and return raw bytes.\n",
    "    \"\"\"\n",
    "    print(f\"🎙️ Recording for {duration_sec} seconds...\")\n",
    "\n",
    "    audio = sd.rec(\n",
    "        int(duration_sec * sample_rate),\n",
    "        samplerate=sample_rate,\n",
    "        channels=1,\n",
    "        dtype=\"int16\",\n",
    "        device=device,\n",
    "    )\n",
    "    sd.wait()\n",
    "\n",
    "    print(\"✅ Done recording.\")\n",
    "    return audio.tobytes()\n",
    "\n",
    "\n",
    "def transcribe_audio_vosk(audio_bytes: bytes) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe audio bytes using Vosk.\n",
    "    \"\"\"\n",
    "    recognizer = KaldiRecognizer(model, sample_rate)\n",
    "\n",
    "    if recognizer.AcceptWaveform(audio_bytes):\n",
    "        result = json.loads(recognizer.Result())\n",
    "        print(\"Result:\", result)\n",
    "        text = result.get(\"text\", \"\")\n",
    "    else:\n",
    "        result = json.loads(recognizer.FinalResult())\n",
    "        print(\"FinalResult:\", result)\n",
    "        text = result.get(\"text\", \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "import scipy.io.wavfile\n",
    "\n",
    "\n",
    "audio_bytes = record_audio(duration_sec=4.0, sample_rate=sample_rate, device=device)\n",
    "\n",
    "# audio_array = np.frombuffer(audio_bytes, dtype=np.int16)\n",
    "# scipy.io.wavfile.write(\"out.wav\", sample_rate, audio_array)\n",
    "\n",
    "transcription = transcribe_audio_vosk(audio_bytes=audio_bytes)\n",
    "\n",
    "transcription"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palm-9000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
