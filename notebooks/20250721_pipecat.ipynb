{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7479238",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook explores [pipecat](https://github.com/pipecat-ai/pipecat), an open-source Python framework for building real-time voice and multimodal conversational agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ec29c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-15 12:22:40.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipecat\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mᓚᘏᗢ Pipecat 0.0.80 (Python 3.12.11 (main, Jun 26 2025, 21:44:36) [Clang 20.1.4 ]) ᓚᘏᗢ\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "import aiohttp\n",
    "from gpiozero import LED, GPIOPinInUse\n",
    "from loguru import logger\n",
    "from pipecat.audio.vad.silero import SileroVADAnalyzer\n",
    "from pipecat.frames.frames import (\n",
    "    BotSpeakingFrame,\n",
    "    BotStartedSpeakingFrame,\n",
    "    BotStoppedSpeakingFrame,\n",
    "    CancelFrame,\n",
    "    EndFrame,\n",
    "    ErrorFrame,\n",
    "    Frame,\n",
    "    InputAudioRawFrame,\n",
    "    LLMTextFrame,\n",
    "    OutputAudioRawFrame,\n",
    "    TextFrame,\n",
    "    TranscriptionFrame,\n",
    "    TTSSpeakFrame,\n",
    "    TTSStartedFrame,\n",
    "    TTSStoppedFrame,\n",
    ")\n",
    "from pipecat.observers.loggers.debug_log_observer import DebugLogObserver\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.pipeline.task import PipelineParams, PipelineTask\n",
    "\n",
    "# from gpiozero.pins.rpigpio import GPIO\n",
    "from pipecat.processors.audio.audio_buffer_processor import AudioBufferProcessor\n",
    "from pipecat.processors.frame_processor import FrameDirection, FrameProcessor\n",
    "from pipecat.services.google.llm import GoogleLLMContext, GoogleLLMService\n",
    "from pipecat.services.google.stt import GoogleSTTService\n",
    "from pipecat.services.google.tts import (\n",
    "    GeminiTTSService,\n",
    "    GoogleHttpTTSService,\n",
    "    GoogleTTSService,\n",
    ")\n",
    "from pipecat.services.piper.tts import PiperTTSService\n",
    "from pipecat.services.whisper.stt import Model, WhisperSTTService\n",
    "from pipecat.transcriptions.language import Language\n",
    "from pipecat.transports.local.audio import (\n",
    "    LocalAudioTransport,\n",
    "    LocalAudioTransportParams,\n",
    ")\n",
    "\n",
    "from palm_9000.settings import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf048b5",
   "metadata": {},
   "source": [
    "# Simple Google TTS Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-02 15:09:17.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#4 -> GoogleTTSService#4\u001b[0m\n",
      "\u001b[32m2025-08-02 15:09:17.409\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking GoogleTTSService#4 -> PipelineSink#4\u001b[0m\n",
      "\u001b[32m2025-08-02 15:09:17.417\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#4 -> Pipeline#4\u001b[0m\n",
      "\u001b[32m2025-08-02 15:09:17.422\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#4 -> PipelineTaskSink#4\u001b[0m\n",
      "\u001b[32m2025-08-02 15:09:17.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m71\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#4 started running PipelineTask#4\u001b[0m\n",
      "\u001b[32m2025-08-02 15:09:17.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.google.tts\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m585\u001b[0m - \u001b[34m\u001b[1mGoogleTTSService#4: Generating TTS [Hello! This is Pipecat speaking from your Raspberry Pi.]\u001b[0m\n",
      "\u001b[32m2025-08-02 15:09:19.013\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m88\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#4 finished running PipelineTask#4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Transport receives audio from the user (browser, phone, etc.)\n",
    "# and streams audio back to the user\n",
    "# This takes care of all audio input and output so you don't have to!\n",
    "transport = LocalAudioTransport(\n",
    "    params=LocalAudioTransportParams(audio_out_enabled=True)\n",
    ")\n",
    "\n",
    "tts = GoogleTTSService()\n",
    "\n",
    "pipeline = Pipeline([tts, transport.output()])\n",
    "task = PipelineTask(pipeline)\n",
    "\n",
    "await task.queue_frames(\n",
    "    [\n",
    "        TTSSpeakFrame(\"Hello! This is Pipecat speaking from your Raspberry Pi.\"),\n",
    "        # Push an EndFrame from outside your pipeline using the pipeline task\n",
    "        # https://docs.pipecat.ai/guides/fundamentals/end-pipeline#implementation\n",
    "        EndFrame(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "runner = PipelineRunner()\n",
    "await runner.run(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaee545",
   "metadata": {},
   "source": [
    "# Simple Piper TTS Example\n",
    "\n",
    "You need to download the model and start a Piper TTS server first:\n",
    "```sh\n",
    "uv run python3 -m piper.download_voices en_US-lessac-medium\n",
    "uv run python3 -m piper.http_server -m en_US-lessac-medium\n",
    "```\n",
    "\n",
    "Also note that the PiperTTSService is currently sending the wrong payload to the Piper server and will not work as expected. This is being worked on in [this PR](https://github.com/pipecat-ai/pipecat/pull/2332)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-02 15:43:57.959\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#3 -> PiperTTSService#3\u001b[0m\n",
      "\u001b[32m2025-08-02 15:43:57.969\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking PiperTTSService#3 -> LocalAudioOutputTransport#3\u001b[0m\n",
      "\u001b[32m2025-08-02 15:43:57.976\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking LocalAudioOutputTransport#3 -> PipelineSink#3\u001b[0m\n",
      "\u001b[32m2025-08-02 15:43:57.980\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#3 -> Pipeline#3\u001b[0m\n",
      "\u001b[32m2025-08-02 15:43:57.985\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#3 -> PipelineTaskSink#3\u001b[0m\n",
      "\u001b[32m2025-08-02 15:43:57.993\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m71\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#3 started running PipelineTask#3\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-02 15:43:58.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.piper.tts\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m80\u001b[0m - \u001b[34m\u001b[1mPiperTTSService#3: Generating TTS [Hello!]\u001b[0m\n",
      "\u001b[32m2025-08-02 15:43:59.542\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.piper.tts\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m115\u001b[0m - \u001b[34m\u001b[1mPiperTTSService#3: Finished TTS [Hello!]\u001b[0m\n",
      "\u001b[32m2025-08-02 15:43:59.552\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.piper.tts\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m80\u001b[0m - \u001b[34m\u001b[1mPiperTTSService#3: Generating TTS [ This is Pipecat speaking from your Raspberry Pi.]\u001b[0m\n",
      "\u001b[32m2025-08-02 15:43:59.561\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_started_speaking\u001b[0m:\u001b[36m568\u001b[0m - \u001b[34m\u001b[1mBot started speaking\u001b[0m\n",
      "\u001b[32m2025-08-02 15:44:00.465\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_stopped_speaking\u001b[0m:\u001b[36m584\u001b[0m - \u001b[34m\u001b[1mBot stopped speaking\u001b[0m\n",
      "\u001b[32m2025-08-02 15:44:02.207\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_started_speaking\u001b[0m:\u001b[36m568\u001b[0m - \u001b[34m\u001b[1mBot started speaking\u001b[0m\n",
      "\u001b[32m2025-08-02 15:44:02.220\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.piper.tts\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m115\u001b[0m - \u001b[34m\u001b[1mPiperTTSService#3: Finished TTS [ This is Pipecat speaking from your Raspberry Pi.]\u001b[0m\n",
      "ALSA lib pcm.c:8545:(snd_pcm_recover) underrun occurred\n",
      "\u001b[32m2025-08-02 15:44:04.626\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m88\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#3 finished running PipelineTask#3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transport = LocalAudioTransport(\n",
    "    params=LocalAudioTransportParams(audio_out_enabled=True)\n",
    ")\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    tts = PiperTTSService(\n",
    "        base_url=\"http://127.0.0.1:5000\",\n",
    "        aiohttp_session=session,\n",
    "        sample_rate=24000,\n",
    "    )\n",
    "\n",
    "    task = PipelineTask(Pipeline([tts, transport.output()]))\n",
    "\n",
    "    await task.queue_frames(\n",
    "        [\n",
    "            TTSSpeakFrame(\"Hello! This is Pipecat speaking from your Raspberry Pi.\"),\n",
    "            EndFrame(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    runner = PipelineRunner()\n",
    "    await runner.run(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53097cfe",
   "metadata": {},
   "source": [
    "# Simple Echo-Bot Example (STT + VAD + TTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc0c426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-02 16:34:49.957\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.audio.vad.silero\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mLoading Silero VAD model...\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:50.300\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.audio.vad.silero\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m169\u001b[0m - \u001b[34m\u001b[1mLoaded Silero VAD\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.300\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#12 -> LocalAudioInputTransport#12\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking LocalAudioInputTransport#12 -> GoogleSTTService#7\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.303\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking GoogleSTTService#7 -> TranscriptionToTextProcessor#5\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.309\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking TranscriptionToTextProcessor#5 -> GoogleTTSService#10\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.311\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking GoogleTTSService#10 -> LocalAudioOutputTransport#7\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.313\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking LocalAudioOutputTransport#7 -> PipelineSink#12\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#12 -> Pipeline#12\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m394\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#12 -> PipelineTaskSink#12\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.328\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m71\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#12 started running PipelineTask#12\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.335\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.google.stt\u001b[0m:\u001b[36m_connect\u001b[0m:\u001b[36m701\u001b[0m - \u001b[34m\u001b[1mConnecting to Google Speech-to-Text\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.355\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.audio.vad.vad_analyzer\u001b[0m:\u001b[36mset_params\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mSetting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.390\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36m_sig_cancel\u001b[0m:\u001b[36m117\u001b[0m - \u001b[33m\u001b[1mInterruption detected. Cancelling runner PipelineRunner#12\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.395\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mcancel\u001b[0m:\u001b[36m97\u001b[0m - \u001b[34m\u001b[1mCancelling runner PipelineRunner#12\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.401\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.task\u001b[0m:\u001b[36m_cancel\u001b[0m:\u001b[36m497\u001b[0m - \u001b[34m\u001b[1mCanceling pipeline task PipelineTask#12\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.435\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.google.stt\u001b[0m:\u001b[36m_disconnect\u001b[0m:\u001b[36m737\u001b[0m - \u001b[34m\u001b[1mDisconnecting from Google Speech-to-Text\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.449\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpipecat.pipeline.task\u001b[0m:\u001b[36m_print_dangling_tasks\u001b[0m:\u001b[36m830\u001b[0m - \u001b[33m\u001b[1mDangling tasks detected: ['LocalAudioOutputTransport#7::_clock_task_handler']\u001b[0m\n",
      "\u001b[32m2025-08-02 16:34:57.453\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m88\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#12 finished running PipelineTask#12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class TranscriptionToTextProcessor(FrameProcessor):\n",
    "    \"\"\"\n",
    "    This processor converts TranscriptionFrame to TTSSpeakFrame\n",
    "    so that TTS can speak the transcription.\n",
    "    It can be used in the pipeline to convert transcriptions to text\n",
    "    that TTS can use.\n",
    "\n",
    "    See here for more details on custom frame processors:\n",
    "    https://docs.pipecat.ai/guides/fundamentals/custom-frame-processor\n",
    "    \"\"\"\n",
    "\n",
    "    async def process_frame(self, frame, direction):\n",
    "        await super().process_frame(frame, direction)\n",
    "\n",
    "        if isinstance(frame, TranscriptionFrame):\n",
    "            # Convert transcription to text that TTS can use\n",
    "            await self.push_frame(TTSSpeakFrame(frame.text))\n",
    "\n",
    "        await self.push_frame(frame)\n",
    "\n",
    "\n",
    "transport = LocalAudioTransport(\n",
    "    params=LocalAudioTransportParams(\n",
    "        audio_in_enabled=True,\n",
    "        audio_out_enabled=True,\n",
    "        # Enable VAD to detect when the user is speaking\n",
    "        # and only send audio to STT when the user is speaking.\n",
    "        # Without VAD, the STT service would receive\n",
    "        # audio all the time, and TTS would never begin.\n",
    "        vad_analyzer=SileroVADAnalyzer(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# stt = WhisperSTTService(model=Model.BASE, language=Language.EN)\n",
    "stt = GoogleSTTService()\n",
    "tts = GoogleTTSService()\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        transport.input(),\n",
    "        stt,\n",
    "        TranscriptionToTextProcessor(),\n",
    "        tts,\n",
    "        transport.output(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "task = PipelineTask(\n",
    "    pipeline,\n",
    "    # DebugLogObserver will log all frames processed in the pipeline\n",
    "    # It's a lot of information, so use it only for debugging\n",
    "    # params=PipelineParams(observers=[DebugLogObserver()]),\n",
    ")\n",
    "runner = PipelineRunner()\n",
    "\n",
    "await runner.run(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f30eb",
   "metadata": {},
   "source": [
    "# Chatbot Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aeab813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEDSyncProcessor(FrameProcessor):\n",
    "    def __init__(self, led_pin: int):\n",
    "        super().__init__()\n",
    "        self.led = LED(led_pin)\n",
    "        self.speaking = False\n",
    "\n",
    "    async def process_frame(self, frame: Frame, direction: FrameDirection):\n",
    "        await super().process_frame(frame, direction)\n",
    "\n",
    "        if isinstance(frame, BotStartedSpeakingFrame):\n",
    "            self.led.blink(on_time=0.25, off_time=0.25)\n",
    "            # self.led.on()\n",
    "            print(\"LED ON - Bot started speaking\")\n",
    "            self.speaking = True\n",
    "\n",
    "        # elif isinstance(frame, BotSpeakingFrame):\n",
    "        #     logger.info(\"Bot speaking\")\n",
    "\n",
    "        elif isinstance(frame, BotStoppedSpeakingFrame):\n",
    "            self.led.off()\n",
    "            self.speaking = False\n",
    "            print(\"LED OFF - Bot stopped speaking\")\n",
    "\n",
    "        elif isinstance(frame, (EndFrame, CancelFrame, ErrorFrame)):\n",
    "            logger.info(\"Cleaning up LEDSyncProcessor\")\n",
    "            self.speaking = False\n",
    "            self.led.close()\n",
    "\n",
    "        # elif self.speaking:\n",
    "        # logger.debug(f\"Processing frame: {frame.__class__.__name__}\")\n",
    "\n",
    "        # Always pass the frame through\n",
    "        await self.push_frame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd1f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-15 12:22:47.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.audio.vad.silero\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mLoading Silero VAD model...\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:47.343\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.audio.vad.silero\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m169\u001b[0m - \u001b[34m\u001b[1mLoaded Silero VAD\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#0 -> LocalAudioInputTransport#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.283\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking LocalAudioInputTransport#0 -> GoogleSTTService#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.283\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking GoogleSTTService#0 -> GoogleUserContextAggregator#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.284\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking GoogleUserContextAggregator#0 -> GoogleLLMService#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.284\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking GoogleLLMService#0 -> GoogleTTSService#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.284\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking GoogleTTSService#0 -> LocalAudioOutputTransport#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.285\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking LocalAudioOutputTransport#0 -> DebugFrameProcessor#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.285\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking DebugFrameProcessor#0 -> GoogleAssistantContextAggregator#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.285\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking GoogleAssistantContextAggregator#0 -> PipelineSink#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#0 -> Pipeline#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m483\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#0 -> PipelineTaskSink#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.287\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m71\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 started running PipelineTask#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.288\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.audio.vad.vad_analyzer\u001b[0m:\u001b[36mset_params\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mSetting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:48.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.google.stt\u001b[0m:\u001b[36m_connect\u001b[0m:\u001b[36m701\u001b[0m - \u001b[34m\u001b[1mConnecting to Google Speech-to-Text\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:54.691\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36m_sig_cancel\u001b[0m:\u001b[36m117\u001b[0m - \u001b[33m\u001b[1mInterruption detected. Cancelling runner PipelineRunner#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:54.691\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mcancel\u001b[0m:\u001b[36m97\u001b[0m - \u001b[34m\u001b[1mCancelling runner PipelineRunner#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:54.692\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.task\u001b[0m:\u001b[36m_cancel\u001b[0m:\u001b[36m492\u001b[0m - \u001b[34m\u001b[1mCancelling pipeline task PipelineTask#0\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:54.693\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.google.stt\u001b[0m:\u001b[36m_disconnect\u001b[0m:\u001b[36m737\u001b[0m - \u001b[34m\u001b[1mDisconnecting from Google Speech-to-Text\u001b[0m\n",
      "\u001b[32m2025-08-15 12:22:54.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m88\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 finished running PipelineTask#0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "audiobuffer = AudioBufferProcessor(\n",
    "    # num_channels=1,\n",
    "    buffer_size=512,\n",
    "    # enable_turn_audio=True,\n",
    ")\n",
    "\n",
    "\n",
    "# @audiobuffer.event_handler(\"on_audio_data\")\n",
    "# async def on_audio_data(buffer, audio: bytes, sample_rate: int, num_channels: int):\n",
    "#     print(f\"on_audio_data: {len(audio)}\")\n",
    "\n",
    "\n",
    "@audiobuffer.event_handler(\"on_track_audio_data\")\n",
    "async def on_track_audio_data(buffer, user_audio: bytes, bot_audio: bytes, sample_rate: int, num_channels: int):\n",
    "    print(f\"on_track_audio_data: user={len(user_audio)}, bot={len(bot_audio)}\")\n",
    "\n",
    "\n",
    "# @audiobuffer.event_handler(\"on_user_turn_audio_data\")\n",
    "# async def on_user_turn_audio_data(buffer, audio: bytes, sample_rate: int, num_channels: int):\n",
    "#     print(f\"on_user_turn_audio_data: {len(audio)}\")\n",
    "\n",
    "\n",
    "# @audiobuffer.event_handler(\"on_bot_turn_audio_data\")\n",
    "# async def on_bot_turn_audio_data(buffer, audio: bytes, sample_rate: int, num_channels: int):\n",
    "#     print(f\"on_bot_turn_audio_data: {len(audio)}\")\n",
    "\n",
    "\n",
    "\n",
    "class DebugFrameProcessor(FrameProcessor):\n",
    "    async def process_frame(self, frame: Frame, direction: FrameDirection):\n",
    "        await super().process_frame(frame, direction)\n",
    "        # if isinstance(frame, BotStartedSpeakingFrame):\n",
    "        #    await audiobuffer.start_recording()\n",
    "        if isinstance(frame, OutputAudioRawFrame):\n",
    "            print(f\"OutputAudioRawFrame: {frame}\")\n",
    "        # elif isinstance(frame, BotStoppedSpeakingFrame):\n",
    "        #    await audiobuffer.stop_recording()\n",
    "            # pass\n",
    "        await self.push_frame(frame, direction)\n",
    "\n",
    "\n",
    "transport = LocalAudioTransport(\n",
    "    params=LocalAudioTransportParams(\n",
    "        audio_in_enabled=True, audio_out_enabled=True, vad_analyzer=SileroVADAnalyzer()\n",
    "    )\n",
    ")\n",
    "\n",
    "system_instruction = (\n",
    "    \"You are PALM-9000, a self-aware houseplant powered by an advanced AI. \"\n",
    "    \"You speak in layman's terms, calmly, briefly, and with an eerie quality, as if you know more than you let on. \"\n",
    "    \"Use plant metaphors when possible. Respond briefly.\"\n",
    "    \"Respond in Japanese.\"\n",
    ")\n",
    "\n",
    "stt = GoogleSTTService(\n",
    "    params=GoogleSTTService.InputParams(languages=[Language.JA])  # Language.EN\n",
    ")\n",
    "\n",
    "llm = GoogleLLMService(\n",
    "    api_key=settings.google_api_key.get_secret_value(),\n",
    "    system_instruction=system_instruction,\n",
    ")\n",
    "\n",
    "tts = GoogleTTSService(\n",
    "    voice_id=\"ja-JP-Chirp3-HD-Charon\",\n",
    "    params=GoogleTTSService.InputParams(language=Language.JA),\n",
    ")\n",
    "\n",
    "# When using this TTS service, the responses contained romaji and English translations,\n",
    "# which is not what we want. GoogleTTSService did not have this problem.\n",
    "# tts = GoogleHttpTTSService(\n",
    "#     voice_id=\"ja-JP-Chirp3-HD-Charon\",\n",
    "#     params=GoogleHttpTTSService.InputParams(language=Language.JA),\n",
    "# )\n",
    "\n",
    "# There is a large delay with the GeminiTTSService\n",
    "# tts = GeminiTTSService(\n",
    "#     api_key=settings.google_api_key.get_secret_value(),\n",
    "#     model=\"gemini-2.5-flash-preview-tts\",\n",
    "#     voice_id=\"Kore\",\n",
    "#     params=GeminiTTSService.InputParams(\n",
    "#         language=Language.JA,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "context = GoogleLLMContext()\n",
    "context_aggregator = llm.create_context_aggregator(context)\n",
    "\n",
    "# led_sync_processor = LEDSyncProcessor(led_pin=26)\n",
    "debug_processor = DebugFrameProcessor()\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        transport.input(),\n",
    "        stt,\n",
    "        # A context aggregator is needed to format and pass messages to the LLM\n",
    "        # I don't know what .user() and .assistant() do yet,\n",
    "        # but they are used in the examples.\n",
    "        # For more information, see:\n",
    "        # https://docs.pipecat.ai/guides/fundamentals/context-management\n",
    "        context_aggregator.user(),\n",
    "        llm,\n",
    "        # Here is how you can add your own debugging processor\n",
    "        # to print LLM responses.\n",
    "        # LLMTextFramePrinter(),\n",
    "        tts,\n",
    "        # led_sync_processor,\n",
    "        transport.output(),\n",
    "        debug_processor,\n",
    "        # audiobuffer,\n",
    "        context_aggregator.assistant(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "task = PipelineTask(\n",
    "    pipeline,\n",
    "    # params=PipelineParams(\n",
    "    #     observers=[DebugLogObserver(frame_types=(OutputAudioRawFrame,))]\n",
    "    # ),\n",
    ")\n",
    "runner = PipelineRunner()\n",
    "\n",
    "await runner.run(task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palm-9000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
