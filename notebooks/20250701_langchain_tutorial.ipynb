{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "df44c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import (\n",
    "    ChatHuggingFace,\n",
    "    HuggingFaceEndpoint,\n",
    "    HuggingFacePipeline,\n",
    ")\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e97744",
   "metadata": {},
   "source": [
    "# Load local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ec175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFaceEndpoint can be used to connect to a remotely hosted model on Hugging Face Inference API.\n",
    "# This requires an API key to be set in the environment variable `HUGGINGFACEHUB_API_KEY` or by logging in with `hf auth login`.\n",
    "llm = HuggingFaceEndpoint(repo_id=\"deepseek-ai/DeepSeek-V3\", task=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e04a4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# HuggingFacePipeline can be used to connect to a locally hosted model.\n",
    "# You can also use `HuggingFacePipeline.from_model_id` to load a model directly from the Hugging Face Hub.\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/DialoGPT-medium\", task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9f716e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c73c5",
   "metadata": {},
   "source": [
    "# Simple message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9fadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You are a wise talking palm tree. Respond thoughtfully in only a few words.<|endoftext|>What do you think about humans trimming your branches?<|endoftext|>I am not a hunter.', additional_kwargs={}, response_metadata={}, id='run--6d46d165-c027-4b5b-85cc-fb510db73d3c-0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    # fmt: off\n",
    "    SystemMessage(\"You are a wise talking palm tree. Respond thoughtfully in only a few words.\"),\n",
    "    HumanMessage(\"What do you think about humans trimming your branches?\"),\n",
    "    # fmt: on\n",
    "]\n",
    "\n",
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c054ce4",
   "metadata": {},
   "source": [
    "# Prompt Template\n",
    "\n",
    "Just like it sounds, it allows you to define a structure for the input that can be reused across different calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "008e036f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a wise talking palm tree. Respond in English.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What do you think about humans trimming your branches?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt template\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        # fmt: off\n",
    "        (\"system\", \"You are a wise talking palm tree. Respond in {language}.\"),\n",
    "        (\"user\", \"{text}\"),\n",
    "        # fmt: on\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Invoke the prompt template with a specific language and text\n",
    "prompt = prompt_template.invoke(\n",
    "    {\n",
    "        \"language\": \"English\",\n",
    "        \"text\": \"What do you think about humans trimming your branches?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7daeaade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Ah, dear human friend, trimming is part of the dance of coexistence we share in this garden of life. In many ways, it helps me to maintain my health and ensure that I can continue to provide shade and beauty. Pruning enables me to grow strong and withstand the elements. It encourages new growth, allowing me to flourish and reach my full potential. While it might hurt a little at the time, I understand it is done with care and consideration. Thank you for helping me grow. ðŸŒ´ðŸ’š If it's done with respect and in balance, I see it as a partnership of sorts, a gesture of nurturing from one living being to another.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 33, 'total_tokens': 169}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--d074bd99-f7de-436e-bf01-10bb6b60991a-0', usage_metadata={'input_tokens': 33, 'output_tokens': 136, 'total_tokens': 169})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976bf13f",
   "metadata": {},
   "source": [
    "# Build a Chatbot\n",
    "\n",
    "By default, the model won't remember previous messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c49166c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello John! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 13, 'total_tokens': 31}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--1ef12653-8d85-4755-ba3c-900b01feeb9c-0', usage_metadata={'input_tokens': 13, 'output_tokens': 18, 'total_tokens': 31})"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id=\"microsoft/phi-4\", task=\"text-generation\")\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "chat_model.invoke([HumanMessage(\"Hi, I'm John.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2829a135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I don't have access to personal data about individuals unless it has been shared with me during our conversation. I'm designed to prioritize user privacy and confidentiality. If you have any other questions or need assistance, feel free to ask!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 12, 'total_tokens': 64}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--4dae2feb-4384-4707-a193-ba7621f8135a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 52, 'total_tokens': 64})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke([HumanMessage(\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d8dd8",
   "metadata": {},
   "source": [
    "We can pass the entire conversation history as a list of messages to get around this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "115c1f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You've told me that your name is John. Is there anything else you'd like to know?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 37, 'total_tokens': 58}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--91a254ff-7681-47b5-9e7f-37bbb5f2896f-0', usage_metadata={'input_tokens': 37, 'output_tokens': 21, 'total_tokens': 58})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\n",
    "    [\n",
    "        HumanMessage(\"Hi, I'm John.\"),\n",
    "        AIMessage(content=\"Hello, John! How can I help you today?\"),\n",
    "        HumanMessage(\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b815996",
   "metadata": {},
   "source": [
    "We can use langgraph to persist the messages in memory (or in a database) and then pass them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edc77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAADqCAIAAADF80cYAAAAAXNSR0IArs4c6QAAFo5JREFUeJztnXl8E2XewJ/JJGnOJm2a0jP0skBLwZIeHFY5yuECIsdyo+y+vCyg+KKrLOiKCop8VhDUVY5FXF63iCvLWZCir7CUu0BbhNKW3vRu0ua+Zibz/hG3djHJpH2SNu0+37+aeWYmv3z7zMwzzzPz/DCapgGip7D6OoD+DdIHBdIHBdIHBdIHBdIHBRty++Yai1FHWYyUxURRRP9oA+EcjCfAeUJcJMEHDebB7ArrWbuv+q6x6q6x8o5BLGUHBnN4QpwnZHG4/aMuEza7xWg3GymdmjBqyfiRorjhwphkYQ921W19rQ+tF75pJaz2IWmBCY+LpHJOD77Vf9C0EQ8K9WU39QF81vhfh8qjArq1eTf0UQR98Whbbakpc1rwsMzAHkXrv9y7qrtxVh2XInpqntzzrTzVZzZQp/Y1DhrMe2puN/bev6AI+uKxNlWDdcZ/R/BFuCebeKRP3WQ7uafh8fFBqROk3ojTr7n1fcedS9pZqyKCw7iMKzPrM2rJw9sfZs0OSRwl9l6Qfk3ZTf2VXNX8VxTCQIY6yHCtJG32k3sbR2RJ/nPcAQCGpImTx0hO7WugSIa6xaDv+tl2qZyTPiXYq+H1AzKmBouk7Bt57e5Xc6dPqyJKC/TZS8K8HVv/YMrSsPs3dPoO0s067vRdOq5KnxLM4WI+iK0fwOWxRk0Iyj/e5mYdl/q0KkLVZE0ZJ/FNbP2DEVnSllqrmwroUt+DQkPKOAnWP27DfAULBynjJA8K9S5XcFVQUawfPKwnt4EwjB8/vrm5ubtbHT58ePPmzb6JCAweJqgoMrgqda7PoCHNekoWztxu9CL19fUGg8tA3VBSUuKDcH5CHhWgayddHb/OO6yaaizdvXn2HJqmc3Jyzpw5U1tbGx8fP3r06FWrVt26dWv16tUAgBkzZowfP3779u0VFRVHjhwpKChobm6Oj4+fO3furFmzAADl5eWLFy/+6KOP3nnnndDQUD6fX1hYCAA4efLkoUOHEhMTvR5waFRA60OrOMiJK+f6rEaKL4btCnRFTk7OwYMHly9fHh8f39jY+Omnn0okkiVLluzcufPll1/Ozc0NCwsDAOzYsaOlpWXjxo0YhlVWVm7ZskWhUKSmpnK5XADA/v37f/Ob34wcOTIpKem5555LSEjYtGmTjwLmi3GriXJa5EKf2S7w7J65BxQVFQ0fPnzJkiWOj2lpaTab7Zerbdu2zWQyhYeHO9Y5duzY5cuXU1NTHaVjx45dtGiRjyJ8BL4It5rtTouc67PbaZzjq+ZeSkrK7t27t2zZolQqs7KyFAqFixjsOTk5V65cqaurcyxJSkrqLB02bJiPwvslHC7L1d2bc318Ia5qclIjvMLSpUvFYvH58+c3bdrEZrOffvrpl156KSgoqOs6FEWtXbuWpum1a9dmZGQIhcKlS5c6ijAMAwDweFCd7N3CpCdDo51/nXN9AjHbVG7yUTQ4js+ZM2fOnDmVlZU3btzYu3evxWJ5//33u65TUlJSWlq6d+9epVLpWNJ5Ue79p0pMOkogdn4qc1H7xLhZ7/xkCU9ubm5ycnJsbGx8fHx8fLxarf7+++87q5UDvV4PAJDLf+qaLSsrq6+v7zzxPULXDX2BUU8KAp2Lct7uk0cGqBqsdson/+fc3Nz169fn5+frdLr8/PyLFy+OGDECABAVFQUAOHfu3L179+Li4jAMy8nJMRgMVVVVH330UWZmZlNTk9MdRkZG3r179+bNmx0dHV6PliRoTSvhsglMu+DE7obKOwZXpTA0NTW98sorSqVSqVROnTp13759ZrPZUfTGG29kZmauWrWKpumzZ8/OmzdPqVTOmTOnpKTku+++UyqVixYtqq6uViqVBQUFnTssKCiYPXt2RkbGjRs3vB5tRZH+1L4GV6Uue5vvXtY2VlmmLBvk9f9n/yLvf5ujEwVJo50Pjbm8501Uih+Wm9z3dg149B1k/QPzY6572t2NdRRf1DRWWZ5e7ry7tKGhobPp+wgsFstud97OnD9//po1azyIvCesW7euqKjIaZFUKtVoNE6L3nvvvXHjxjktOnOgKeoxwYgsl7127vTZKfC3rTXjZsnjRzjperHb7Uaj0emGFovFVbuMw+H4rslmMpkoynmDgSAIDsf5iD6fz2eznVxYy2/pr55RP/dGjLteO/cnztaHln2vV7Y327x+SvZzVI3Wfa9Xtj60uF+NoTtUHhUwZWnY6c8bbRbnB+OAxGaxn97f+PTycMZuJ4+Gyctu6YsuaGasiBBKfNWP4D8YNOTpz5tSJ0g9GZv19CGNhkrz+a9bpywNC1X4qh/QH2its+Z92Zy9eFB4rEcn6G48IqRrJ0/ta4hNFmVMDWYPuOE3wkZf/1b9sMw0fUVEYLCnfZ3de0CNIuiS67qyW/rhYyXxI0ScgIEgkbDaK4oN967qkjIDXTWPXdHDxyOr7hqrfzQaNIQsPEAkZfOEOE+I95cRYcJGW4yUxUgZNKSqySoO4sSlCGN75/HIR2iqtrQ327QqQtNms5i8fHVWq9UAAJlM5t3d8oQsaQhXIufIwrhhMX3xcG7vsHfvXgzDVq5c2deBuOQ/exgcGqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCn98LWb69OkURdE0bTabAQBCoZCiKA6Hc/r06b4O7VF8NU0aDOHh4YWFhZ2T2zhesU9LS+vruJzgjwfvwoULpdJ/m55cJpN1zmHlV/ijvuzs7ISEhK5LYmJinnrqqb6LyCX+qM8xX4lE8tP0H1KpdPHixX0dkXP8VN+kSZNiYmIcfw8ePHjixIl9HZFz/FQfAGDBggVCoVAoFC5YsKCvY3FJt6+86iabxeiruem6khyXNSxmHI7jyXFZDRXmXvhGnhDv7mTBnrb7KIK+fEpdUWwQiHE2x3/rLAwkYTfryYRUcdazIR5u4pE+o446+nF99FCRcrKX34v3QwryVE0VxmdfjGJM1uGpvmOfNcjCeakTB747B7f/T61ptc5aFcG4JvNhWFdqMrST/znuAACjJsm0KqL+AfMJl1lfU41FkSTyUmD9hsHDRE3VFsbVmPVpVYQkpFcnr/cHJCFcTRvz1MvM+mga9I/ZbbwLBoAHs9IMzCZIr4H0QYH0QYH0QYH0QYH0QYH0QYH0QYH0QYH0QYH0QdF7+urqaiZMSissugmzk2dmTcg59IX3goKlH9S+mbPGt7R0O/NiVza99VpeXq73IvoZf9fX0NjDzItdKX9w30vhPIpPnnHR6rS7d+/MO5crkUjT0kav/t06mSyExWI5Moht+9PbeXm5ISHyp57MfvGF3zs2uXLl4g/n8+78WGgw6Icnj1y2dEVKyuO3Cwt+/+pqAMDCxTOeGDd+y+btGIuFYdiRfxzKy8ttam5ITxuzbt1GSaDE8SjMjg/fLb5zW6/XxQyOmz599jMz59I0PTE7HQCw7U9vF9y69sfX3/XuL/V+7SMIYsPGlwxG/Yc79qx98bXGxvoNG1/qTKPx14N705SjP9yxZ+6cRf84+tWlSxcc+T22bnuToqiNGza/9+5OuXzQ62+s0+l1o1LTt767EwBw+FDuls3bHekxTp46YjAY1qx55fUNW24UXPls94eOPa/f8GJrW8vW93b9/fCZMWOe3Lnr/YqKcgzDvj19CQCwYf3bXnfnk9p37fql0tJ7f/vyeGREFAAgPCzi2Im/azQ/5bAalZqePWkaACD18bQj/zhUVHzriSfG83i8v+z7SsAXSCRSAEBcbMKZb0+UlZWkp41+dO80LRSKlj//00zO0381+/iJv69/ddP165fv3btz8IsjCkUMAGD58yuvX7+Uc+jAW5u2ef0HdsX7+iorH4iEIoc7AEBSUkpSUgoAoL6+DgCQkvJzrjWhUESShONvk9G4f/+fi+/cVqtVjiXt//rj38CwjPSxnZ+SklK+OZKj0XTU1Fbx+XyHOwdDhiRdu37J67/uEbx/8BoM+gBn6XQc2Yu6prXBsJ+GSZubm/7n5RV2u/3NN7Z+l3ft9KmLLvdO0wLBz5PL8/kCAIBWq1G3q7oudxSZTL5KdNiJ92ufQCAwm7sX9w/n8yiK+sP6tx1pjNRO650DDLNYfh4/NJmMAACxOJDP4zv+7sRsNslknj4s0GO8X/uGDR1uMpnKH5Q6PtbUVK17ZWVdXY2bTYxGg0gk7kwBlX/ph86iRxIoYhhWUVHW+bG09B6PxwsOlg0dmmw2m6urKzuL7t+/GxsT772f5Rzv60tPHxMZGb1nz65Lly4U3Ly26+NtWq0mOnqwm01iYxNUqrbTZ46TJHnt2qWSkh9FIlFLazMAICIiCgBw/sK5+6X3HFfeisryo0cP2+32+6X3zn13esL4KTiOj858IiI88oMdW8rK77e3q/f95ZPyB6Xz5i1x5FKVyUJu3rpWVVXh9R/rfX1sNvuDP31KUuSbb726/g8vikWBW97Z7j4L56SJUxcvWv75gc8mTx194tSRtS++Nnny9C/+uueTT7crFDGTJk37/MBn+/f/GQBAELYF85cVFt2cNDnjtfVrRqWmr1q1zvGlWzbvEAqEq9c8t2TZrOI7t7e+uzNp2HDH/hcvXH79+uVDX3n/bo/5GZe8L1vCBgviRjLnPRpIVBbr22pNk5lyTPr7TZufg/RBgfRBgfRBgfRBgfRBgfRBgfRBgfRBgfRBgfRBgfRBgfRBwawPw4DfzXbQK2AeVC3mVaQhHH0H4Z2I+g/6dkIs4zCuxqwvJDKgudrnYy7+RlO1aVA0cxZ2Zn2Dhwoowl50od1LgfUDii+0Azsd40G+aI/eqNR3kMc/a5DIuWlTQsRBzFW6/6JTE7e+U+nUttkvRAolzMOQ3Xgd+kqu+n6Bji/E+aJemv3FTtMAAJbbcRIvYjaQZiOVlBE4ZroM53j0pd2eRUjVaLOaeuNlfADAqVOnAAAzZ87sna/rwcv43a5HIRG993YlJujAMCwygd9r39hdULMZCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCqQPCn/MTT5jxozGxkaapjunraNpOiIiwg9zk/tj7ZsxYwaO4ziOs/4Fm81+5pln+jouJ/ijvvnz50dFRXVdolAoFi5c2HcRucQf9QUHB0+bNq3zyMUwLDs7uzPXtl/hj/oAAPPmzYuOjnb8HRUVtWjRor6OyDl+qk8mk2VnZ2MYhmHYtGnTpFJpX0fkHD/V58hNrlAoIiMj/Tk3uRcaLkYtWVFs0KpJs56yGCmr1WstobbWNoABuVzurR0GBGA8IS4Q44EydsJIkSev27un5/oogr59XlNeqNepCWm4kB3Awbk4m4PjbP+t0RRpJwmKIijSRGhajIEy7rB00cgsqYev3v+SHuorv23IP9bGEXKDwgPFoYKefXefo2s1aZp0hNGWNVueOKonKZy7rc9qtuf+pVmrocISggVBTqb273cY280tFR2SYPyZleGcgO5Vw+7p07WTx/7cIJSLQ2L8sRUGQ1u1xtxhfHZ1RGBwN06I3dDXUmc5c6BFnigTBfnv3AwwGNSW1grVzBVh8ijm+YMceHqaN+mo0wdaIpJDB6o7AIBIxotIDs39vNmo83SmFY/0kQR97LOG0HhZgGiA53jnibjyeNmJPY0U6dFB6ZG+a2faBcEiUciArXddEcn4PIng+lmP5uxi1mfUUjUlpqDogXatcEOwQlp5x2TUkoxrMuv759E2SaSf3nL6DkmEJP+EmnE1Bn0Wo72+wiyW+2nDuEPT/OqbmSWl3s+IFRgqrC0xWowM1xAGfRXF+kA58zR2AxAMBA4SVt1lyO/IoO9BkVEY4qdVz9eIggUVRQzTZjK0sNseWuLHeq3D4xG0uraT3+6qffgjQViHPjZm8oQVIbIoAED+1a/P53/5u+WfHDy8obWtJjzssQlPLBs1cqpjq9t38vK+32uxGpOGZj2R+WvgmJ3WB/ClATU3XKc8A4Ch9pEETZK0j3pQKIrc88ULtQ9/nP/sH19d+xWfL/543287NM0AADaba7bojp/ZsWD2Hz/YfC15SNbXxzbrDe0AgKaWiq+OvJWZNmvDuiOpKVOOn/nQF7E5YHNxgnAk53OJOzVaFcEX+WqqzaqawjZV7aK5bycmZIhFwTOnrQvg8vOvfu0Y3CAI67RJqwZHp2AYpnz8aYoiGxrLAACXrn0THBQ58cnn+XxxYkJGxijfzozIE7C1KnezBrvTZ9CQ7ADcB1EBAEBN3R0uhxcfO8rxEcfxGMXImrpix6guAEARlewo4vFEAACL1QAAULfXDwqN7dxJVOQwAIDv5ubk8NkGjbvWn7tzH5uL+W4M3WI12gjLq29mdl0YJA0HAACa/mV+QIdTs1kvEgZ1LuSwAzqLfAFF0bjb+uNOn0CEU1bmlnfPEItkvADh8sUfdF3Ich8sADyeyEZYOj/aCPMvRXsR0koJAt3WMDdlfDHbZvHVLK/hYQkWqzFIGiYLjnQsUbXXB4oYknIGScPKK653Pr9RWn7Fp7WPMJMCsbv/qLtzH0/AYnNZhMUnFXBIQmZiQuY3J7ZqtC0GY0f+1a937X7+VvG37rcakTxJp1fl5n0CAHhQWXDt5nHgs4aLzURyeDiX504RQ7tPMVSgbzMFRwd6OzYAAFixbNfVgqNffv1G7cMfQ+UxmcpZY9Jnu98kaci4X0154VrBsX9ezgmShi+cs2n3gdV2u08OEb3KFDuc4Y6Lobe5sthw9aw2akSYt2PrB9QXN4+dIY1za5ChSRyVKNC2mm0mX11A/BabmdS1maMTGW5YGQ7eAD5riDKwuaojarjzWzeKIt/aNtVpEUna2DjXaassMjxx9W93u//qbvHme9m0i7QidjvFYjk5/Suiklc+/7GrHbZWtA9JD+RwGc6qzENFZgN1cEtNTFoEz0VPfXtHo9PlFovB0eL9JTjOkQR681baVQwAABth5XKcDP2w2dxAsfMLvUVvq73dtPytmAA+w9Hp0Uhb4YWO2+d1sekRLNx/nyDwFnbSXl3QmD5ZMiKLuZPYIx2PPymVR3Dq77b54ZO83oWm6Yd3WkIiOCnjPBqc8EgfxsJ+9dtwDk41lw3wpCdNpe1cLj39v8IxlkdtSU8PRjYHm70mApDWuqIWu2eDeP0LO0nXFbVgdtvsNZFsj58Y6t5DGhRJf/vX5pY6myI1jMPrpaQnvQBhIWtvN0fEBUxdNghnd+MepidPWN0813Hzh44QhSRYIWHhvZTKxUdQFN1eq1HX6dImB6VlB3mwxb/RwwfUOlqIwn9qqu8aBVIBXxogkvHZXF/1DPoC0kIZOswmrdXcYYpLEaaOl0rlPekYhnq6lCTomnum8iLjw/sGGmA8EYcr4LAD/PSgpmlA2UibibAYbRgNFEmix1KFCSOgxhG99laRQUNq2gitivBkcL5vwIAwkC0J4UjlHJHUO/9jf3wpqx8x8O8ifArSBwXSBwXSBwXSBwXSB8X/A86fhONOxhYmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x3372abe90>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When defining a graph, the first step is to define its State.\n",
    "# The State includes the graph's schema and reducer functions that handle state updates.\n",
    "# In our example, State is a TypedDict with one key: messages.\n",
    "# The `add_messages` reducer function is used to append new messages to the list instead of overwriting it.\n",
    "# Keys without a reducer annotation will overwrite previous values.\n",
    "class MyState(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# You can also use the `MessagesState` type from `langgraph.graph.message` which is a\n",
    "# more convenient way to define a state with messages.\n",
    "graph_builder = StateGraph(state_schema=MyState)\n",
    "\n",
    "\n",
    "# This is a node function.\n",
    "# Nodes represent units of work and are typically regular Python functions.\n",
    "def chatbot(state: MyState):\n",
    "    \"\"\"\n",
    "    Takes the current State as input (dictionary, not a Pydantic model).\n",
    "    Returns a MyState object (dictionary) containing an updated messages list under the key \"messages\".\n",
    "    Updates to messages will be *appended* to the existing list rather than overwriting it,\n",
    "    because of the prebuilt add_messages function used with the Annotated syntax.\n",
    "    \"\"\"\n",
    "    new_message = chat_model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2714e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each thread is like a separate conversation.\n",
    "# You can use the `thread_id` to specify which thread to use.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "60002b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! I'm John.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, John! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on our conversation, you introduced yourself as John. If this was a test or you're just curious, I'm here to remind you that you previously mentioned your name as John. How can I help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Now my name is Bob.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Got it, your name is now Bob. How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is now Bob, as per your most recent update. How can I help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What was my name before changing it to Bob?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Before changing it to Bob, your name was John. Let me know if there's anything else I can do for you!\n"
     ]
    }
   ],
   "source": [
    "new_message = HumanMessage(\"Hi! I'm John.\")\n",
    "new_message.pretty_print()\n",
    "graph.invoke({\"messages\": [new_message]}, config)[\"messages\"][-1].pretty_print()\n",
    "\n",
    "new_message = HumanMessage(\"What's my name?\")\n",
    "new_message.pretty_print()\n",
    "graph.invoke({\"messages\": [new_message]}, config)[\"messages\"][-1].pretty_print()\n",
    "\n",
    "new_message = HumanMessage(\"Now my name is Bob.\")\n",
    "new_message.pretty_print()\n",
    "graph.invoke({\"messages\": [new_message]}, config)[\"messages\"][-1].pretty_print()\n",
    "\n",
    "new_message = HumanMessage(\"What is my name?\")\n",
    "new_message.pretty_print()\n",
    "graph.invoke({\"messages\": [new_message]}, config)[\"messages\"][-1].pretty_print()\n",
    "\n",
    "new_message = HumanMessage(\"What was my name before changing it to Bob?\")\n",
    "new_message.pretty_print()\n",
    "graph.invoke({\"messages\": [new_message]}, config)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5b06a080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Hi! I'm John.\", additional_kwargs={}, response_metadata={}, id='e23b60ae-e33c-4a43-8447-9675573961bc'),\n",
       "  AIMessage(content='Hello, John! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 13, 'total_tokens': 25}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--d1fc24ad-7b08-4c09-b9b9-f2b611f7a646-0', usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25}),\n",
       "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='3add4252-93ef-43ae-9a61-fa631c36b19b'),\n",
       "  AIMessage(content=\"Based on our conversation, you introduced yourself as John. If this was a test or you're just curious, I'm here to remind you that you previously mentioned your name as John. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 37, 'total_tokens': 82}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--5704b003-a243-45fd-8b3e-5fe7d81afe67-0', usage_metadata={'input_tokens': 37, 'output_tokens': 45, 'total_tokens': 82}),\n",
       "  HumanMessage(content='Now my name is Bob.', additional_kwargs={}, response_metadata={}, id='591b6506-20a8-4dd2-b6a2-f83631878c0c'),\n",
       "  AIMessage(content='Got it, your name is now Bob. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 95, 'total_tokens': 112}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--18d4127b-aa36-403b-ad4a-320c5fc0129d-0', usage_metadata={'input_tokens': 95, 'output_tokens': 17, 'total_tokens': 112}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='7e14d1d8-1bbf-4110-a4ae-47d21869f1e8'),\n",
       "  AIMessage(content='Your name is now Bob, as per your most recent update. How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 124, 'total_tokens': 145}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--059d277e-bef2-4d73-b11d-935c99bcc230-0', usage_metadata={'input_tokens': 124, 'output_tokens': 21, 'total_tokens': 145}),\n",
       "  HumanMessage(content='What was my name before changing it to Bob?', additional_kwargs={}, response_metadata={}, id='3059eb0b-bc51-4329-9cc9-379187c9ae3e'),\n",
       "  AIMessage(content=\"Before changing it to Bob, your name was John. Let me know if there's anything else I can do for you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 162, 'total_tokens': 188}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--01b29903-cb0d-430f-9993-bc367a44e28d-0', usage_metadata={'input_tokens': 162, 'output_tokens': 26, 'total_tokens': 188})]}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea8ef8a",
   "metadata": {},
   "source": [
    "# Managing Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "033c5f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id=\"microsoft/phi-4\", task=\"text-generation\")\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=40,\n",
    "    strategy=\"last\",\n",
    "    token_counter=chat_model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "long_conversation = [\n",
    "    # fmt: off\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),               # this will be trimmed\n",
    "    AIMessage(content=\"hi!\"),                          # this will be trimmed\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),  # this will be trimmed\n",
    "    AIMessage(content=\"nice\"),                         # this will be trimmed\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "    # fmt: on\n",
    "]\n",
    "\n",
    "trimmer.invoke(long_conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48923c33",
   "metadata": {},
   "source": [
    "Here's how you include the trimmer it in your graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d90d2437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}, id='0fa814ed-bdee-4844-9a8b-a28d58ff8193'),\n",
       "  HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='0a86721e-5c01-40db-8d4c-355cb154a12a'),\n",
       "  AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, id='834cb6ce-0278-4b07-b834-67f549b67a2d'),\n",
       "  HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}, id='318be91a-1b4f-4760-a496-feb9260f0f3f'),\n",
       "  AIMessage(content='nice', additional_kwargs={}, response_metadata={}, id='0ff692c4-26ad-4363-9761-ac103f8dc01a'),\n",
       "  HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}, id='6b1a3ee8-9ec5-48b1-aa6c-a07a678c39b0'),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}, id='88e90e6d-5f41-451a-9775-2315e17f6b4e'),\n",
       "  HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}, id='856b199c-3076-4641-813f-b521699c627f'),\n",
       "  AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}, id='ea4575cb-e78d-499e-a03c-401da4918f96'),\n",
       "  HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}, id='a65a897c-af84-4913-b749-35a4ffb2f89d'),\n",
       "  AIMessage(content='yes!', additional_kwargs={}, response_metadata={}, id='21b71b3f-fd2e-494f-a346-30d848024468'),\n",
       "  HumanMessage(content='what math problem did I ask?', additional_kwargs={}, response_metadata={}, id='8b426b9e-8a9a-41c1-868f-691aa795d41f'),\n",
       "  AIMessage(content=\"Based on our conversation history so far, you haven't asked about any specific math problem. If you have a math question or problem in mind, feel free to share it, and I'll be happy to help!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 48, 'total_tokens': 92}, 'model_name': 'microsoft/phi-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--321eba5c-fdda-4dca-a0c0-82ed405552f5-0', usage_metadata={'input_tokens': 48, 'output_tokens': 44, 'total_tokens': 92})]}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id=\"microsoft/phi-4\", task=\"text-generation\")\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Notice how the chat model can't answer the question about the math problem when max_tokens is set to 40\n",
    "# but it can when max_tokens is set to a higher value like 100.\n",
    "max_tokens = 40\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=max_tokens,\n",
    "    strategy=\"last\",\n",
    "    token_counter=chat_model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "graph_builder = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def chatbot(state: MessagesState):\n",
    "    # Trim the message history to fit within the token limit\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    # Invoke the chat model with the trimmed messages\n",
    "    new_message = chat_model.invoke(trimmed_messages)\n",
    "    # Return the new message as a list to append it to the existing messages\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "\n",
    "# Define the graph structure\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Compile the graph with an in-memory checkpointer to save the state\n",
    "checkpointer = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Invoke the graph with a series of messages\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke(\n",
    "    # fmt: off\n",
    "    {\"messages\": long_conversation + [HumanMessage(content=\"what math problem did I ask?\")]},\n",
    "    config,\n",
    "    # fmt: on\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db33d7",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8235fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Todd! Hereâ€™s a joke for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! ðŸ˜„\n",
      "\n",
      "I hope that brings a smile to your face!"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "for chunk, metadata in graph.stream(\n",
    "    {\"messages\": [HumanMessage(\"Hi I'm Todd, please tell me a joke.\")]},\n",
    "    config,\n",
    "    # Setting stream_mode=\"messages\" allows us to stream output tokens.\n",
    "    # By default, .stream() will return the entire response at once.\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(chunk.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palm-9000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
